# -*- coding: utf-8 -*-
"""Part3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f8XezpEgIPy-LxmC-94vL4qZl5COQ9Jh
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Function to create a closed-form solution for Ridge regression
class RidgeClosedForm:
    def __init__(self, l2_penalty=1.0):
        self.l2_penalty = l2_penalty
        self.coef_ = None
        self.intercept_ = None

    def fit(self, X_train, y_train):
        n_samples, n_features = X_train.shape

        # Add bias column for the intercept (this is X_0 = 1 for all samples)
        X_b = np.column_stack([np.ones((n_samples, 1)), X_train])

        # Compute the closed-form solution (theta = (X^T X + lambda * I)^(-1) X^T y)
        I = np.eye(n_features + 1)  # Create identity matrix for regularization
        I[0, 0] = 0  # Don't regularize the intercept term

        # Closed-form solution for Ridge regression
        self.coef_ = np.linalg.inv(X_b.T @ X_b + self.l2_penalty * I) @ X_b.T @ y_train

    def predict(self, X_test):
        n_samples = X_test.shape[0]
        X_b = np.column_stack([np.ones((n_samples, 1)), X_test])  # Add bias term
        return X_b @ self.coef_

# Intelligent rounding function
def dynamic_rounding(y_pred, threshold=0.5, min_value=0, max_value=5):
    y_pred = np.clip(y_pred, min_value, max_value)
    fractional_part = y_pred - np.floor(y_pred)
    return np.where(fractional_part < threshold, np.floor(y_pred), np.ceil(y_pred))



def manual_basis_expansion(X, degree=2, add_log=True, add_exp=True, add_poly=True):

    n_samples, n_features = X.shape
    features = [X]  # Start with the original features

    # Polynomial features (degree 2)
    if add_poly:
        for i in range(n_features):
            for j in range(i, n_features):
                features.append(X[:, i] * X[:, j])  # Interaction terms

        # If higher degree polynomial features are needed
        if degree > 2:
            for d in range(2, degree + 1):
                for i in range(n_features):
                    features.append(X[:, i] ** d)

    # Safeguard for log: Apply only to positive values
    if add_log:
        for i in range(n_features):
            X_safe_log = np.where(X[:, i] > 0, X[:, i], 1e-6)  # Replace non-positive values with a small number
            features.append(np.log(X_safe_log + 1))  # Add log-transformed feature

    # Safeguard for exp: Clip large values to avoid overflow
    if add_exp:
        for i in range(n_features):
            X_safe_exp = np.clip(X[:, i], -100, 100)  # Clip to avoid overflow
            features.append(np.exp(X_safe_exp))  # Add exp-transformed feature

    # Return the expanded feature matrix
    return np.column_stack(features)


# Load dataset
data = pd.read_csv('train.csv')

# Split features and target
X = data.drop(columns=['ID', 'score']).values
y = data['score'].values

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Standardize features
# X_train_scaled = manual_standard_scaler(X_train)
# X_test_scaled = manual_standard_scaler(X_test)

# Apply basis expansion
X_train_expanded = manual_basis_expansion(X_train)
X_test_expanded = manual_basis_expansion(X_test)

# Check for NaN values and replace with zero or mean if found
X_train_expanded = np.nan_to_num(X_train_expanded, nan=0)
X_test_expanded = np.nan_to_num(X_test_expanded, nan=0)

# Set L2 regularization penalty (lambda)
l2_penalty = 100.0

# Train closed-form Ridge regression
ridge_reg = RidgeClosedForm(l2_penalty=l2_penalty)
ridge_reg.fit(X_train_expanded, y_train)

# Predict using the closed-form solution
y_pred = ridge_reg.predict(X_test_expanded)

# Apply intelligent rounding to match the required integer output
y_pred_rounded = dynamic_rounding(y_pred)

# Calculate MSE for rounded predictions
mse_loss_rounded = mean_squared_error(y_test, y_pred_rounded)
print(f"Mean Squared Error after dynamic rounding: {mse_loss_rounded}")

import pandas as pd

# Step 1: Read the test dataset
df1 = pd.read_csv('test.csv')

# Step 2: Extract features from the test dataset
X_test = df1.drop(columns=['ID']).values

# Step 3: Standardize features

X_test_expanded = manual_basis_expansion(X_test)
X_test_expanded = np.nan_to_num(X_test_expanded, nan=0)

y_pred = ridge_reg.predict(X_test_expanded)

# Step 4: Apply intelligent rounding
y_pred_rounded = dynamic_rounding(y_pred)

# Step 7: Create a new DataFrame with ID and rounded predicted scores
output_df = pd.DataFrame({
    'ID': df1['ID'],
    'score': y_pred_rounded
})

# Step 8: Save the predictions to a CSV file
output_df.to_csv('kaggle.csv', index=False)

print("Predictions saved to submission.csv successfully!")

# from google.colab import files

# # Download the file named 'kaggle.csv'
# files.download('kaggle.csv')

